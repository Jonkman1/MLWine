---
title: "MachinelearningonWine"
author: "Harrie"
date: "12/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
library(tidyverse)
library(finalfit)
library(tidymodels)
library(caret)
library(rpart)   
library(randomForest)

```




# INTRODUCTION
`Tidymodels` is the successor to the `caret` package which is used during the Introduction to Data Science course of the Harvard University. `Tidymodels` is a collection of modeling packages that, like the `tidyverse`, have consistent API and are designed to work together specifically to support predictive analytics and machine learning. Core tidymodel packages include: parsnip, recipes, rsample and tune. Collectively, these packages provide a grammar for modeling that makes things a lot easier and provide a unified modeling and analysis interface to seamlessly access several model varieteis in R. (References xxx) 
Different packages


Last months I tried to learn working with `tidymodels`. I tried to finish the Capstone course with the use of this packages. 


# PROBLEM DEFINITION
Data mining approaches are use to predict human wine taste preferences that are based on easily available analytical tests at certification step. A dataset on red wine from Portugal is used here to research quality of the wine and different predictors (Cortez et al., 2009) Supervised machine learning support us in this. In this world two kind of algorithms are often used. One is called regression and the other is called classification. In this study we use regression for predicting quality of wine based on the predictors.


The wine data used here contains the following independent and dependent variables

Independent variables: (symbol I)
- I1 Fixed acidity (g(tartaric acid/dm3)
- I2 Volatile acidity (g(acetic acid)/dm3)
- I3 Citric acid (g/dm3)
- I4 Residual sugar (g/dm3)
- I5 Chlorides (g(sodium chloride)/dm3)
- I6 Free sulfar dioxide (mg/dm3)
- I7 Total sulfar dioxide (mg/dm3)
- I8 Density (g/cm3)
- I9 pH
- I10 Sulphates (g(potassium sulphate)/dm3)
- I11 Alcohol (vol%)

Dependent variable: (symbol D)
- D1 Quality


# DATA LOADING AND PREPROCESSING 

Let us first load the dataset.

```{r}
wf<-readRDS("wine.rds")
```

Let us give a summary of the data frame.

```{r}
glimpse(wf)
```

Do we see missings?

```{r}
missing_glimpse(wf)
```
What kind of variables do we have and what are their scores?

```{r}
ff_glimpse(wf)
```

Let us make correlation matrix of the data.

```{r}
cor(wf)
```

Let us define the column names on a consistent way.

```{r}
colnames(wf) <- wf %>% 
  colnames() %>% str_replace_all(pattern = " ", replacement = "_")
colnames(wf)
```

Let us also remove any missing values.

```{r}
wf <- na.omit(wf)

```


## EXPLORATIVE DATA ANALYSIS (EDA).

Now we have preprocessed the data we explore them.  let us first visualise correlations within the dataset. For this you need the package `corrplot`

```{r}
library(corrplot)
wf %>% cor() %>% 
  corrplot.mixed(upper = "circle",
                 tl.cex = 1,
                 tl.pos = 'lt',
                 number.cex = 0.75)
```



## SPLITTING THE DATA
Now we split the data into: a) Train set, b) Test set. Here we work on the last pre-model analysis. All functions below come from the `rsample` package, which is part of `tidymodels`. First we set the seed to fix the randomisation and to make reproducable possible. We use 80% of the dataset for the trainingset. We split it and than make a training- and test-dataset

```{r}
set.seed(12345) 

data_split <- initial_split(wf, prop = 0.8) 

train_data <- training(data_split)

test_data  <- testing(data_split)

```


# MODELING AND DATA ANALYSIS 

## 1. Lineair modelling
For the continues outcome or target variable `quality` we first research some different lineair regression models and choose the best one. For the below tasks, we store each formula in a different R object.

We have to define the data: 
- The target variable. `quality` is the target variable and it is numeric
- The features of the model (predictors) are the other variables here and they are numeric also.

Futhermore, we esign a simple formula to predict the target variable. In this formula (f1) all the available 11 predictors are used.

```{r}
formula <- formula(quality ~ fixed_acidity + volatile_acidity + citric_acid + 
                   residual_sugar + chlorides + free_sulfur_dioxide + 
                   total_sulfur_dioxide + density + pH + sulphates + alcohol)
```


Let us fit a linear regression model to the data. What we do:
- We created an object that will store the model fit.    
- We specify the model.   
- We specify also that we work with regression because of the continue target variable (`quality`)
- We specify also the `lm` package to train the model
- We add the formula and the training data to fit the model

```{r}
lm_fit <- 
  linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm") %>% 
  fit(formula, data = train_data) 
```

Show the results

```{r}
print(lm_fit$fit)
```

Show the results in another way.

```{r}
summary(lm_fit$fit)
```

We can also visualise the fit summary by using the `broom` package which is inside `tidymodels`.

```{r}
tidy(lm_fit$fit) %>% mutate_if(is.numeric, round, 3)
```


## 2. Decision tree
After we worked with lineair regression, it is possible to work with other models which maybe give us better results. Let us first look at decision tree. For this you need decision tree and for this you have to install and open the library of `rpart`. We see similar steps here
- defining an object `dt_fit`   
- telling that we work with decision tree   
- once again set the mode on regression   
- once again set the engine on `rpart`   
- fit the formula on the training data-set   

```{r}

dt_fit <- 
  decision_tree() %>% 
  set_mode("regression") %>%
  set_engine("rpart") %>% 
  fit(formula, data = train_data)
```

Print the results

```{r}
print(dt_fit$fit)
```

As a sidestep we can visualise this, but than we have to install and open the `visNetwork` and `sparkline` packages. Then we see this.

```{r}
library(visNetwork)
library(sparkline)
visTree(dt_fit$fit)
```

## 3. Random forest
A third model we use here is RandomForest.You need to install `randomForest` and open the library `randomForest`. And again the same steps:
- define object `rf_fit`    
- tell we want to use randomforst    
- set the mode again on regression      
- set the engine here on randomForest    
- fit the model on the training_set   

```{r}
rf_fit <- 
  rand_forest() %>% 
  set_mode("regression") %>%
  set_engine("randomForest") %>% 
  fit(formula, data = train_data)
```

Print these results.

```{r}
print(rf_fit$fit)
```



# EVALUATION AND PREDICTION

Now we have three objects which we have to evaluate. We do this on the test-set. We compare three models (`lm_fit`, `dt_fit` and `rf_fit`) on the MSE score.

## Accuracy of the lm-model

```{r}
lm_pred <- test_data %>% 
  bind_cols(predict(object = lm_fit, new_data = test_data))
```

Now we see a new column, `.pred`, with a predicted scores for each row. 

```{r}
View(lm_pred)

```

```{r}
lm_pred <- test_data %>% 
  bind_cols(predict(object = lm_fit, new_data = test_data)) %>% 
  mutate(pred = round(.pred, 0))

```


```{r}
lm_mse <- lm_pred %>% 
  summarise(type = "lm",
            MSE = round(mean((pred - quality)^2), 4))
```


```{r}
head(lm_mse)
```


## Accuracy of the Decision Tree Model

```{r}
dt_pred <- test_data %>% 
  bind_cols(predict(object = dt_fit, new_data = test_data)) %>% 
  rename(pred = .pred) %>% 
  mutate(pred = round(pred, 0))
```

```{r}
dt_mse <- dt_pred %>% 
  summarise(type = "dt",
            MSE = round(mean((pred - quality)^2), 4))
```


```{r}
head(dt_mse)
```

## Accuracy of the Random Forest Model

```{r}
rf_pred <- test_data %>% 
  bind_cols(predict(object = rf_fit, new_data = test_data)) %>% 
  rename(pred = .pred) %>% 
  mutate(pred = round(pred, 0))
```


```{r}
rf_mse <- rf_pred %>% 
  summarise(type = "rf",
            MSE = round(mean((pred - quality)^2), 4))
```


```{r}
head(rf_mse)
```




## All results together

Let us put all the results together.

```{r}
res <- bind_rows(lm_mse, dt_mse, rf_mse)
```


Let us show these results.

```{r}
head(res)
```
We choose the random_forest model as the best opportunity here. Let us look at it once again.

```{r}
View(rf_pred)
```

```{r}
head(rf_pred)
```


# RESULTS SUMMARIZED


In this simple scenario, we may be interested in seeing how the model performs on the testing data that was left out. The code below will fit the model to the training data and apply it to the testing data. There are other ways we could have done this, but the way we do it here will be useful when we start using more complex models where we need to tune model parameters.

After the model is fit and applied, we collect the performance metrics and display them and show the predictions from the testing data.

```{r}
head(rf_pred)
```
```{r}
metrics(rf_pred, quality, pred)
```

# CONCLUDING REMARKS

[](https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c)

# References
Attalides, N. (2020). [Introduction to machine learning. Barcelona. Presentation](https://www.barcelonar.org/workshops/BarcelonaR_Introduction_to_Machine_Learning.pdf) 

Barter, R.(2020). [Tidymodels: tidy machine learning in R](http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/) 

Baumer, B. Kaplan, D.T., Horton, N.J. (2017). *Modern data science with R*. CRCPress: Boca Raton.

Boehmke, B. & Greenwell, B. (2020). *Hands on machine learning with R*. [Bookdown version](https://bradleyboehmke.github.io/HOML)

Cortez, P., Cerdeira, A., Almeida, F., Matos, T. & Reis, J. (2009). Modeling wine prefernces by data mining from physicochemical properties. *Decision Support Systems, 47*, 547-533. 

Hartie, T., Tibskirani, R. & Friedmann, J. (2009). *The elements of statistical learning. Data mining, inference and prediction*. 2nd edition. Springer: New York.

Irizarry, R.A. (2020). *Introduction to data science. Data analysis and prediction algorithms with R*. CRC Press: Boca Raton.

James, S., Witten, D., Hastie, T.. & Tibskirani, R. (2013). *An introduction to statistical learning with application in R*. 

Jonkman, H. (2019-2021). [Harrie's hoekje, his website with different blogs on machine learning in Dutch](http://www.harriejonkman.nl/HarriesHoekje/) 

Kuhn, M. 7 Johnson, K. (2013). *Applied predictive modeling*. Springer: New York.

Kuhn, M. & Johnson, K. (2019). [Feature engineering and selection: A practical approach for predictive models](www.feat.engineering)

Lendway, L. (2020). 2020_north-tidymodels.[Introduction on github](https://github.com/llendway/2020_north_tidymodels)

Lewis, J.E. (2020). [Coding machine learning models](https://www.youtube.com/watch?v=WlL44_is4TU)

Raoniar, R. (2021). Modeling binary logistic regression using tidymodels library in R (Part 1). [Towards data science](https://towardsdatascience.com/modelling-binary-logistic-regression-using-tidymodels-library-in-r-part-1-c1bdce0ac055)

Ruiz, E. (2019). [A gentle introduction to tidymodels](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/)

Seyedian, A. (2021). [Medical cost personal datasets. Insurance forcast by using linear regression](https://www.r-bloggers.com/2021/02/using-tidymodels-to-predict-health-insurance-cost/)

Silge, J. (2020). [Get started with tidymodels and #TidyTuesdag Palmer penguins](https://juliasilge.com/blog/palmer-penguins/)

Silge, J. (2021). [Supervised machine learning case studies in R](https://supervised-ml-course)

Tidymodels. [Tidymodels website](https://www.tidymodels.org/)

