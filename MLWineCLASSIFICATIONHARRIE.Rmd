---
title: "Machine learning on Wine data. CLASSIFICATION"
author: "Harrie"
date: "12/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
library(tidyverse)
library(tidymodels)
library(caret)
library(rpart)   
library(randomForest)
library(visdat)
library(GGally)
library(gt)
library(skimr)
library(corrplot)
library(visNetwork)
library(sparkline)

```


# INTRODUCTION
`Tidymodels` is the successor to the `caret` package which is used during the Introduction to Data Science course of the Harvard University. `Tidymodels` is a collection of modeling packages that, like the `tidyverse`, have consistent API and are designed to work together specifically to support predictive analytics and machine learning. Core tidymodel packages include: parsnip, recipes, rsample and tune. Collectively, these packages provide a grammar for modeling that makes things a lot easier and provide a unified modeling and analysis interface to seamlessly access several model varieteis in R. (References xxx) 
Different packages


Last months I tried to learn working with `tidymodels`. I tried to finish the Capstone course with the use of this packages. 


# PROBLEM DEFINITION
Data mining approaches are use to predict human wine taste preferences that are based on easily available analytical tests at certification step. A dataset on red wine from Portugal is used here to research quality of the wine and different predictors (Cortez et al., 2009) Supervised machine learning support us in this. In this world two kind of algorithms are often used. One is called regression and the other is called classification. In this study we use regression for predicting quality of wine based on the predictors.


The wine data used here contains the following independent and dependent variables

Independent variables: (symbol I)
- I1 Fixed acidity (g(tartaric acid/dm3)
- I2 Volatile acidity (g(acetic acid)/dm3)
- I3 Citric acid (g/dm3)
- I4 Residual sugar (g/dm3)
- I5 Chlorides (g(sodium chloride)/dm3)
- I6 Free sulfar dioxide (mg/dm3)
- I7 Total sulfar dioxide (mg/dm3)
- I8 Density (g/cm3)
- I9 pH
- I10 Sulphates (g(potassium sulphate)/dm3)
- I11 Alcohol (vol%)

Dependent variable: (symbol D)
- D1 Quality


# DATA LOADING AND PREPROCESSING 

Let us first load the dataset.

```{r}
wf<-readRDS("wine.rds")
```

Let us give a summary of the data frame.

```{r}
glimpse(wf)
```
2.2 Clean the data

To get a first impression of the data we take a look at the top 4 rows:


```{r}
library(gt)

wf %>% 
  slice_head(n = 4) %>% 
  gt() # print output using gt
```

2.3 Format data

Next, we take a look at the data structure and check wether all data formats are correct:

- Numeric variables should be formatted as integers (int) or double precision floating point numbers (dbl).

- Categorical (nominal and ordinal) variables should usually be formatted as factors (fct) and not characters (chr). Especially, if they don’t have many levels.



```{r}
glimpse(wf)
```

The package `visdat` helps us to explore the data class structure visually:


```{r}
library(visdat)

vis_dat(wf)
```

2.4 Missing data


Now let’s turn our attention to missing data. Missing data can be viewed with the function vis_miss from the package visdat. We arrange the data by columns with most missingness:

```{r}
vis_miss(wf, sort_miss = TRUE)
```


Create new variables
One very important thing you may want to do at the beginning of your data science project is to create new variable combinations. For example here I want to work with classification and change the contiunues variable `quality` to categorical variable `quality_two`


```{r}

percentage <- prop.table(table(wf$quality)) * 100
cbind(freq=table(wf$quality), percentage=percentage)
```

With `recipe` package we can do a lot at the same time

```{r}
set.seed(123)

wdNA<-
  wf %>%
  mutate(
    # Convert quality to a factor
    quality = ifelse(quality >5, "high", "low"),
    quality = factor(quality)
)
  
```


Take out missing data.

```{r}
wdNA<-na.omit(wdNA)

```
`


Check the missings


```{r}
vis_miss(wdNA, sort_miss = TRUE)
```

  
  
Let us look at the dependent variable closely.

```{r}

percentage <- prop.table(table(wdNA$quality)) * 100
cbind(freq=table(wdNA$quality), percentage=percentage)
```

```{r}
wdNA %>%
  ggplot(aes(quality)) +
  geom_bar() +
  ggtitle("Quality of wine, Low (0) and High (1)")
```


## 2.6 Fix column names

```{r}
colnames(wdNA) <- wdNA %>% 
  colnames() %>% str_replace_all(pattern = " ", replacement = "_")
colnames(wdNA)
```


## 2.7 Data overview
```{r}
library(skimr)
skim(wdNA)
```


We have:

-

```{r}
library(GGally)
wdNA %>%
  ggscatmat(alpha=0.2)
```

Another option is to use ggpairs:

```{r}
wdNA %>%
  ggpairs()
```

## A summary of the data frame

```{r}
summary(wdNA)
glimpse(wdNA)
```


## EXPLORATIVE DATA ANALYSIS (EDA).


```{r}
library(corrplot)
wdNA %>%
   select(-quality) %>%
  cor() %>% 
  corrplot.mixed(upper = "circle",
                 tl.cex = 1,
                 tl.pos = 'lt',
                 number.cex = 0.75)
```


## Split the data into: a) Train set, b) Test set

All functions below come from the `rsample` package

```{r}
set.seed(12345) # to fix randomisation by setting the seed (reproducibility)

data_split <- initial_split(wdNA, prop = 0.8) # Use 80% of the data for training

train_data <- training(data_split)

test_data  <- testing(data_split)

```


## Validation set

Remember that we already partitioned our data set into a training set and test set. This lets us judge whether a given model will generalize well to new data. However, using only two partitions may be insufficient when doing many rounds of hyperparameter tuning (which we don’t perform in this tutorial but it is always recommended to use a validation set).

Therefore, it is usually a good idea to create a so called validation set. Watch this short video from Google’s Machine Learning crash course to learn more about the value of a validation set.

We use k-fold crossvalidation to build a set of 5 validation folds with the function vfold_cv. We also use stratified sampling:

```{r}
set.seed(100)

wine_cv <-
 vfold_cv(train_data, 
          v = 5, 
          strata = quality) 
```

We will come back to the validation set after we specified our models

# MODELING AND DATA ANALYSIS 

## Prepare models

```{r}
wdNA_rec <-
  recipe(quality ~ .,
         data=train_data) %>%
         prep()
```

```{r}
wdNA_rec
```

## Execute pre-processing
The testing data can now be transformed using the exact same steps, weights, and categorization used to pre-process the training data. To do this, another function with a cooking term is used: bake(). Notice that the testing() function is used in order to extract the appropriate data set.

```{r}
wdNATEST <-wdNA_rec %>%
  bake(testing(data_split))
```

```{r}
glimpse(wdNATEST)
```

Performing the same operation over the training data is redundant, because that data has already been prepped. To load the prepared training data into a variable, we use juice(). It will extract the data from the iris_recipe object.

```{r}
wdNATRAING <- juice(wdNA_rec)
glimpse(wdNATRAING)
```


## Model training
The process of specifying our models is always as follows:

Pick a model type   
- set the engine  
- Set the mode: regression or classification   
- You can choose the model type and engine from this list.

### Logistic regression

We run the model and show the estimates.

```{r}
log_spec <- logistic_reg() %>%  
  set_engine(engine = "glm") %>%  
  set_mode("classification") %>% 
  fit(quality~., data=train_data)

  tidy(log_spec)
```
Here we write down the results in Odds-ratio's which are better to understand.

```{r}
tidy(log_spec, exponentiate=TRUE)
```

Let us show the same results but only the signficant predictors on quality.

```{r}
tidy(log_spec, exponentiate=TRUE) %>%
  filter(p.value < 0.05)
```

# Model prediction

Now it is time to use the test data. Let us show the class predictions for the first 5 cases.

```{r}
pred_class<-predict(log_spec,
                       new_data=test_data,
                       type="class")

pred_class[1:5,]
```

Test data class probabilities (the predicted probabilities for the first five cases) 

```{r}
pred_proba<-predict(log_spec,
                    new_data=test_data,
                    type="prob")


pred_proba[1:5,]
```



Here we see the final data preparation for model evaluation

```{r}
quality_results<-test_data %>%
  select(quality) %>%
  bind_cols(pred_class, pred_proba)

quality_results[1:5,]
```


## Model evaluation
Let us evaluate the defined model. For this we use the confusion matrix. 120 case were high and predicted as such and 116 cases were low and predicted as such. 37 wre low but predicted as high and 45 had a true high score but predicted as low. 

```{r}
conf_mat(quality_results, truth = quality,
         estimate = .pred_class)
```

Let us now research the accuracy of the model.

```{r}
accuracy(quality_results, truth=quality, 
         estimate=.pred_class)
```


What can we say about the sensitivity of the model. 
```{r}
sens(quality_results, truth=quality, estimate = .pred_class)
```

What about the specificity of the model.

```{r}
spec(quality_results, truth=quality, estimate = .pred_class)
```


Now these three scores together.

```{r}
custom_metrics<-metric_set(accuracy, sens, spec)

custom_metrics(quality_results,
               truth=quality,
               estimate=.pred_class)
```



# RESULTS SUMMARIZED
Let us summarize the results. And it is easy to use the good and old `caret` package.

```{r}
library(caret)

confusionMatrix(quality_results$.pred_class,
                quality_results$quality,
                pos="high")
```




# References
Attalides, N. (2020). [Introduction to machine learning. Barcelona. Presentation](https://www.barcelonar.org/workshops/BarcelonaR_Introduction_to_Machine_Learning.pdf) 

Barter, R.(2020). [Tidymodels: tidy machine learning in R](http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/) 

Baumer, B. Kaplan, D.T., Horton, N.J. (2017). *Modern data science with R*. CRCPress: Boca Raton.

Boehmke, B. & Greenwell, B. (2020). *Hands on machine learning with R*. [Bookdown version](https://bradleyboehmke.github.io/HOML)

Cortez, P., Cerdeira, A., Almeida, F., Matos, T. & Reis, J. (2009). Modeling wine prefernces by data mining from physicochemical properties. *Decision Support Systems, 47*, 547-533. 

Hartie, T., Tibskirani, R. & Friedmann, J. (2009). *The elements of statistical learning. Data mining, inference and prediction*. 2nd edition. Springer: New York.

Irizarry, R.A. (2020). *Introduction to data science. Data analysis and prediction algorithms with R*. CRC Press: Boca Raton.

James, S., Witten, D., Hastie, T.. & Tibskirani, R. (2013). *An introduction to statistical learning with application in R*. 

Jonkman, H. (2019-2021). [Harrie's hoekje, his website with different blogs on machine learning in Dutch](http://www.harriejonkman.nl/HarriesHoekje/) 

Kuhn, M. 7 Johnson, K. (2013). *Applied predictive modeling*. Springer: New York.

Kuhn, M. & Johnson, K. (2019). [Feature engineering and selection: A practical approach for predictive models](www.feat.engineering)

Lendway, L. (2020). 2020_north-tidymodels.[Introduction on github](https://github.com/llendway/2020_north_tidymodels)

Lewis, J.E. (2020). [Coding machine learning models](https://www.youtube.com/watch?v=WlL44_is4TU)

Raoniar, R. (2021). Modeling binary logistic regression using tidymodels library in R (Part 1). [Towards data science](https://towardsdatascience.com/modelling-binary-logistic-regression-using-tidymodels-library-in-r-part-1-c1bdce0ac055)

Ruiz, E. (2019). [A gentle introduction to tidymodels](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/)

Seyedian, A. (2021). [Medical cost personal datasets. Insurance forcast by using linear regression](https://www.r-bloggers.com/2021/02/using-tidymodels-to-predict-health-insurance-cost/)

Silge, J. (2020). [Get started with tidymodels and #TidyTuesdag Palmer penguins](https://juliasilge.com/blog/palmer-penguins/)

Silge, J. (2021). [Supervised machine learning case studies in R](https://supervised-ml-course)

Tidymodels. [Tidymodels website](https://www.tidymodels.org/)






